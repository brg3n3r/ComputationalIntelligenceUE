\documentclass{article}
\usepackage[a4paper,left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{latexsym,amsfonts,amsmath,amssymb,amstext,graphicx,titlesec,ae,aecompl,mathtools,tabularx, multirow, cancel, nicefrac,subcaption, blindtext, floatrow}
\setlength{\parindent}{0pt}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]


\begin{document}

\begin{titlepage}
       \begin{center}
             \begin{huge}
				   %% Update assignment number here
                   \textbf{Assignment 2}
             \end{huge}
       \end{center}

       \begin{center}
             \begin{large}
                   Computational Intelligence, SS2020
             \end{large}
       \end{center}

       \begin{center}
 \begin{tabularx}{\textwidth}{|>{\hsize=.33\hsize}X|>{\hsize=.33\hsize}X|>{\hsize=.33\hsize}X|} 

                   \hline
                   \multicolumn{3}{|c|}{\textbf{Team Members}} \\
                   \hline
                   Last name & First name & Matriculation Number \\
                   \hline
                   Blöcher & Christian & 01573246 \\
                   \hline
                   Bürgener & Max & 01531577 \\
                   \hline
                    &  &  \\
                   \hline

             \end{tabularx}
       \end{center}
\end{titlepage}

\section{Linear regression}

\subsection{Derivation of Regularized Linear Regression}

\begin{itemize}

%    \begin{figure}[h]
%        \centering
%        \includegraphics[width=\textwidth]{./Figures/scenario2_findexponential.pdf}
%        \caption{Scenario 2 with mixed measurement models for the anchors}
%        \label{fig:scenario2_findexponential}
%    \end{figure}
    
    \item Why is the design matrix $\boldsymbol{X}$ containing $n + 1$ and not just $n$?
    
    \begin{itemize} 
    
        \item The simplest form of linear regression is calculated by:
            $$y(\boldsymbol{x, \omega}) = \underset{bias}{\underbrace{\omega_0}} + \boldsymbol{\omega}^T \boldsymbol{w}$$
        \item The additional column is used to compensate the sum of the bias $\omega_0$ and thus to simplify our calculation. \\
      
            $$ \boldsymbol{X} = 
            \begin{bmatrix}
	            1		& \dots		& x_1	\\
	            \vdots	& \ddots 	& \vdots		\\
	            1		& \dots 		& x_N
	            \end{bmatrix}
	            \begin{bmatrix}
	            \omega_0 \\
	            \vdots	 \\
	            \omega_N	 
            \end{bmatrix} $$
     \end{itemize}
    
    \item What is the dimension of the gradient vector?
    
    \begin{itemize}
        \item The gradient contains the derivation of all $J\boldsymbol{(\theta)}$ for all variables. That means the gradient has the dimension $m \times 1$ and it contains a group of targets. \\        
            $$\frac{\partial J(\boldsymbol{\boldsymbol{\theta)}}}{\partial \boldsymbol{\theta}} =
            \begin{bmatrix}
            	\frac{\partial J(\boldsymbol{\theta)}}{\partial \theta_1}	 \\
            	\vdots \\
            	\frac{\partial J(\boldsymbol{\theta)}}{\partial \theta_m}  	
            \end{bmatrix}$$     
    \end{itemize} 
    
    \item What is the definition of the Jacobian matrix and what is the difference between the gradient and the Jacobian matrix?
    
    \begin{itemize}
        \item The Jacobian matrix elements are calculated by the derivatives of all outputs of a vector with respect to all parameters.  \\
            $$ \boldsymbol{J} = 
            \begin{bmatrix}
            	\frac{\partial y_1}{\partial x_1}	&	\dots	&	\frac{\partial y_1}{\partial x_n} 	\\
            	\vdots								&	\ddots	&	\vdots								\\
            	\frac{\partial y_m}{\partial x_1	}	&	\dots	&	\frac{\partial y_m}{\partial x_n}
            \end{bmatrix} $$
            
        \item The gradient is computed from a scalar. It is a vector which contains the derivatives of a function for all parameters.              
    \end{itemize}
    
    \item What is the dimension of the Jacobian matrix and what is it equal to?
    
    \begin{itemize}
    
        \item The dimension of the Jacobian matrix equals the dimension of the Design matrix
    
    \end{itemize}
    
    \item Minimization of the regularized linear regression cost function
    
    $$\begin{array}{rclr}
        J(\boldsymbol{\theta}) & = & \frac{1}{m} \| \boldsymbol{X\theta - y} \|^2 + \frac{\lambda}{m} \| \boldsymbol{\theta}^2 \| & \\\\
        J(\boldsymbol{\theta}) & = & \frac{1}{m} \left[(\boldsymbol{X\theta - y})^T \cdot (\boldsymbol{X\theta - y}) \right] + \frac{\lambda}{m} (\boldsymbol{\theta^T \theta}) & \mid \frac{\partial}{\partial\boldsymbol{\theta}} \\
      \end{array}$$
    
      \textrm{Using Hint 2 from our exercise sheet the calculation is simplified to:} \\
    
    $$\begin{array}{rclr}
        \frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} & = & \frac{2}{m}(\boldsymbol{X\theta - y})^T \cdot \boldsymbol{X} + \frac{2\lambda}{m}\ \boldsymbol{\theta}^T & \mid \overset{!}{=} 0
    \end{array}$$

    We set the gradient to zero to calculate our solution $\theta$
    
    $$\begin{array}{rclr}    
    \frac{2}{m}(\boldsymbol{\theta}^T \boldsymbol{X}^T \boldsymbol{X} - \boldsymbol{y}^T \boldsymbol{X}) + \frac{2\lambda}{m}\boldsymbol{\theta}^T & = & 0 \\\\
    \boldsymbol{\theta}^T \boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{\theta}^T & = & \boldsymbol{y}^T \boldsymbol{X} & \\\\
    \boldsymbol{\theta}^T (\boldsymbol{X}^T \boldsymbol{X} + \lambda E) & = & \boldsymbol{y}^T\boldsymbol{X} & \\\\
    \boldsymbol{\theta}^T & = & (\boldsymbol{y}^T \boldsymbol{X})(\boldsymbol{X}^T \boldsymbol{X} + \lambda \boldsymbol{E})^{-1} & \mid ()^T \\\\
    \end{array}$$
	
	Due to the property of transpose: $ (\boldsymbol{A}\boldsymbol{B})^T = \boldsymbol{B}^T\boldsymbol{A}^T $ we get:
	
	$$\begin{array}{rcl}
	
    \theta & = & (\boldsymbol{X}^T\boldsymbol{X} + \lambda\boldsymbol{E})^{-1^T} (\boldsymbol{y}^T\boldsymbol{X})^T \\\\
    \theta & = & (\boldsymbol{X}^T\boldsymbol{X} + \lambda\boldsymbol{E})^{-1} (\boldsymbol{X}^T\boldsymbol{y})
	\end{array}$$
    
\end{itemize}

\newpage

\subsection{Linear Regression with polynomial features}


%\begin{figure}[!ht]
%\centering
%\makebox[\textwidth]{
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_poly1_deg1.pdf}
%\caption{$n=1$}
%\end{subfigure}
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_poly1_deg5.pdf}
%\caption{$n=5$}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_poly1_deg10.pdf}
%\caption{$n=10$}
%\end{subfigure}
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_poly1_deg22.pdf}
%\caption{$n=22$}
%\end{subfigure}
%}
%\caption{Results of Linear Regression for varying polynomial degree $n$.}
%\label{linreg_poly1}
%\end{figure}
%
%\begin{figure}[!ht]
%\makebox[\textwidth]{
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_poly2_besttrain.pdf}
%\caption{Training set, $n=30$}
%\end{subfigure}
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_poly3_bestval.pdf}
%\caption{Validation set, $n=13$}
%\end{subfigure}
%}
%\caption{Results with lowest cost on training and validation set.}
%\label{linreg_poly23}
%\end{figure}
%
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=0.8\textwidth]{./Figures/linreg_poly4_errors.pdf}
%\caption{Training, validation and testing costs as a function of the polynomial degree $n$.}
%\label{linreg_poly4}
%\end{figure}

As can be seen in figure \ref{linreg_poly4} the training cost decreases with increasing polynomial degree $n$. That means the lowest cost on the training set can be achieved by using polynomials up to the highest degree $n=30$ (s. figure \ref{linreg_poly23}), but then the testing cost is maximised. This is due to overfitting: By finding parameters that suit the training data best, the solution becomes too specific for the validation and testing sets, leading to greater errors. Looking at figure \ref{linreg_poly23} all training data points are in the close vicinity of the polynomial but the output function barely resembles the data. The best results can be obtained with degree $n=13$, which minimises the validation cost (s. figure \ref{linreg_poly4}) and leads to a very low testing cost. Having a validation set in addition to the training set helps in finding the right degree for the linear regression process and greatly improves the quality of the solution.

\newpage

\subsection{Linear Regression with polynomial features}


\subsection{(Bonus) Linear Regression with radial basis functions}

%\begin{figure}[!ht]
%\centering
%\makebox[\textwidth]{
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_rbf1_ncent1.pdf}
%\caption{$l=1$}
%\end{subfigure}
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_rbf1_ncent5.pdf}
%\caption{$l=5$}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_rbf1_ncent10.pdf}
%\caption{$l=10$}
%\end{subfigure}
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_rbf1_ncent22.pdf}
%\caption{$l=22$}
%\end{subfigure}
%}
%\caption{Results of Linear Regression for varying degree $l$.}
%\label{linreg_rbf1}
%\end{figure}
%
%\begin{figure}[!ht]
%\makebox[\textwidth]{
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_rbf2_besttrain.pdf}
%\caption{Training set, $l=40$}
%\end{subfigure}
%\begin{subfigure}{0.65\textwidth}
%\includegraphics[width=\textwidth]{./Figures/linreg_rbf3_bestval.pdf}
%\caption{Validation set, $l=9$}
%\end{subfigure}
%}
%\caption{Results with lowest cost for training and validation set.}
%\label{linreg_rbf23}
%\end{figure}
%
%\begin{figure}[!ht]
%\centering
%\includegraphics[width=0.8\textwidth]{./Figures/linreg_rbf4_errors.pdf}
%\caption{Training, validation and testing costs as a function of the degree $l$.}
%\label{linreg_rbf4}
%\end{figure}

The results of the Linear Regression approach with radial basis functions are similar to the ones with polynomial functions: Increasing the degree/number of kernels $l$ leads to lower training cost (s. figure \ref{linreg_rbf4}) - minimised for $l=40$ (s. figure \ref{linreg_rbf23}) - but choosing $l$ too high results in overfitting (s. figure \ref{linreg_rbf1}). The cost function of the validation set is minimised for degree $l=9$ (s. figure \ref{linreg_rbf23}), leading to a low but not quite minimised testing cost. The actual testing cost minimum is found with degree $l=10$. Improving on the results of Linear Regression with polynomial basis functions it is about $20\%$ lower than the testing cost minimum with (higher) degree $n=13$ (s. figures \ref{linreg_rbf1} and \ref{linreg_poly23}). 


\clearpage

\section{Logistic Regression}

\subsection{Derivation of Gradient}

$$\begin{array}{rccl}
    J(\boldsymbol{\theta}) & = & - \frac{1}{m	} \displaystyle \sum_{i=1}^{m} \left(y^{(i)} \ \textrm{log}(h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)})) + (1 - y^{(i)}) \ \textrm{log} (1 - h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)}))\right) & \mid \frac{\partial}{\partial \boldsymbol{\theta}_j} \\\\
\end{array}$$
    
Derivation of the hypothesis function $ h_{\boldsymbol{\theta}}(\boldsymbol{x}) $:\

$$\begin{array}{rccl}  

&&\frac{\partial h_{\boldsymbol{\theta}}(\boldsymbol{x})}{\partial \theta_j} = \frac{\partial}{\partial \theta_j} \sigma(\theta_0 x_0 + \theta_1 x_1 + \dots + \theta_n x_n) = x^{(i)}_j &\\\\
\end{array}$$

Using the chain rule and the given hint for the derivation of the sigmoid function $\sigma$, we get:

$$\begin{array}{rccl}       
    \frac{J(\boldsymbol{\theta})}{\partial \theta_j} & = & -\frac{1}{m} \displaystyle \sum_{i=1}^{m}\left( y^{(i)} \frac{1}{h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)})} h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)}) \left(1 - h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)})\right) \ x^{(i)}_j + \dots \right.  &  \\\\
    & & \left. \dots (1 - y^{(i)}) \frac{1}{1 - h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)})} \left(-h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)}) \ (1 - h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)})) \ x^{(i)}_j\right) \right) \\\\
    \frac{J(\boldsymbol{\theta})}{\partial \theta_j} & = & -\frac{1}{m} \displaystyle \sum_{i=1}^{m}\left(y^{(i)} (1 - h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)}) + h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)})) - h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)}) \right) x^{(i)}_j & \\\\
    \frac{J(\boldsymbol{\theta})}{\partial \theta_j} & = & \frac{1}{m} \displaystyle \sum_{i=1}^{m} (y^{(i)} + h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)}) \ x^{(i)}_j
    
\end{array}$$

\newpage

\subsection{Logistic Regression training with gradient descent}

\begin{itemize}
\item

\item

%\begin{figure}[!ht]
%\centering
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg2_iter20_error.pdf}
%\caption{Error, 20 iterations}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg2_iter20_decbound.pdf}
%\caption{Decision boundary, 20 iterations}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg2_iter2000_error.pdf}
%\caption{Error, 2000 iterations}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg2_iter2000_decbound.pdf}
%\caption{Decision boundary, 2000 iterations}
%\end{subfigure}
%}
%\caption{GD errors and decision boundaries for varying number of iterations, degree $l=1$ and learning rate $\eta=1$.}
%\label{logreg2}
%\end{figure}
%
%\item
%
%\begin{figure}[!ht]
%\centering
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg3_eta01_error.pdf}
%\caption{Error, $\eta=0.1$}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg3_eta01_decbound.pdf}
%\caption{Decision boundary, $\eta=0.1$}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg3_eta2_error.pdf}
%\caption{Error, $\eta=2$}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg3_eta2_decbound.pdf}
%\caption{Decision boundary, $\eta=2$}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg3_eta20_error.pdf}
%\caption{Error, $\eta=20$}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg3_eta20_decbound.pdf}
%\caption{Decision boundary, $\eta=20$}
%\end{subfigure}
%}
%\caption{GD errors and decision boundaries for varying learning rate $\eta$, degree $l=2$ and 200 iterations.}
%\label{logreg3}
%\end{figure}

\item

%\begin{figure}[!ht]
%\vspace*{-53.6pt}
%\centering
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg1_opt_error.pdf}
%\caption{Error, $l=1$, $\eta=1$, 60 iterations}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg1_opt_decbound.pdf}
%\caption{Decision boundary, $l=1$, $\eta=1$, 60 iterations}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg2_opt_error.pdf}
%\caption{Error, $l=2$, $\eta=5$, 75 iterations}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg2_opt_decbound.pdf}
%\caption{Decision boundary, $l=2$, $\eta=5$, 75 iterations}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg7_opt_error.pdf}
%\caption{Error, $l=7$, $\eta=10$, 1000 iterations}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg7_opt_decbound.pdf}
%\caption{Decision boundary, $l=7$, $\eta=10$, 1000 iterations}
%\end{subfigure}
%}
%
%\makebox[\textwidth]{
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg20_opt_error.pdf}
%\caption{Error, $l=20$, $\eta=8$, 1500 iterations}
%\end{subfigure}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{./Figures/logreg4_deg20_opt_decbound.pdf}
%\caption{Decision boundary, $l=20$, $\eta=8$, 1500 iterations}
%\end{subfigure}
%}
%\caption{GD errors and decision boundaries for degrees $l=1,2,7,20$ and chosen parameter values.}
%\label{logreg4_fig}
%\end{figure}
%
%\begin{table}[!ht]
%\centering
%\begin{tabular}{|c||c|c||c|c|}
%\hline
%degree $l$ & learning rate $\eta$ & iterations & training cost & testing cost \\ \hline \hline
%1          & 1                    & 60         & 0.470         & 0.391        \\ \hline
%2          & 5                    & 75         & 0.450         & 0.354        \\ \hline
%7          & 10                   & 1000       & 0.310         & 0.361        \\ \hline
%20         & 8                    & 1500       & 0.297         & 0.398        \\ \hline
%\end{tabular}
%\caption{Chosen values for learning rate $\eta$ and number of iterations and obtained training and testing costs for degrees $l=1,2,7,20$.}
%\label{logreg4_tab}
%\end{table}

\item

\end{itemize}

\end{document}