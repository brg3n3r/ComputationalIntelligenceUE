\documentclass{article}
\usepackage[a4paper,left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{latexsym,amsfonts,amsmath,amssymb,amstext,graphicx,titlesec,ae,aecompl,mathtools,tabularx, multirow, cancel, nicefrac,subcaption, blindtext, floatrow}
\setlength{\parindent}{0pt}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]


\begin{document}

\begin{titlepage}
       \begin{center}
             \begin{huge}
				   %% Update assignment number here
                   \textbf{Assignment 4}
             \end{huge}
       \end{center}

       \begin{center}
             \begin{large}
                   Computational Intelligence, SS2020
             \end{large}
       \end{center}

       \begin{center}
 \begin{tabularx}{\textwidth}{|>{\hsize=.33\hsize}X|>{\hsize=.33\hsize}X|>{\hsize=.33\hsize}X|} 

                   \hline
                   \multicolumn{3}{|c|}{\textbf{Team Members}} \\
                   \hline
                   Last name & First name & Matriculation Number \\
                   \hline
                   Blöcher & Christian & 01573246 \\
                   \hline
                   Bürgener & Max & 01531577 \\
                   \hline
                    &  &  \\
                   \hline

             \end{tabularx}
       \end{center}
\end{titlepage}


\section{Linear SVM}

\subsection{Plots}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=.6\textwidth]{./Figures/1_data}
	\caption{Dataset}
	\label{Dataset}
\end{figure}

\begin{figure}[!ht]
	\makebox[\textwidth]{
	\begin{subfigure}{0.6\textwidth}
	\includegraphics[width=\textwidth]{./Figures/1a_bound.pdf}
	\caption{Dataset}
	\end{subfigure}
	\begin{subfigure}{0.6\textwidth}
	\includegraphics[width=\textwidth]{./Figures/1b_bound}
	\caption{Dataset with additional data point}
	\end{subfigure}
	}	
	\caption{Classification of the dataset using SVM}
	\label{SVM_classification}
\end{figure}

\begin{figure}[!ht]
    \centering
    \makebox[\textwidth]{
    \begin{subfigure}{0.6\textwidth}
    \includegraphics[width=\textwidth]{./Figures/1c_bound_C1.pdf}
    \caption{$C=1$}
    \end{subfigure}
    \begin{subfigure}{0.6\textwidth}
    \includegraphics[width=\textwidth]{./Figures/1c_bound_C01.pdf}
    \caption{$C=0,1$}
    \end{subfigure}
    }
    
    \makebox[\textwidth]{
    \begin{subfigure}{0.6\textwidth}
    \includegraphics[width=\textwidth]{./Figures/1c_bound_C0001.pdf}
    \caption{$C=0,001$}
    \end{subfigure}
    \begin{subfigure}{0.6\textwidth}
    \includegraphics[width=\textwidth]{./Figures/1c_bound_C1e6.pdf}
    \caption{$C=1000000$}
    \end{subfigure}
    }
    \caption{SVM Classification using different C parameters}
    \label{SVM_Cs}
\end{figure}

\subsection{How and why changed the decision boundary when the new point was added?}

The margin is defined as the perpendicular distance between the decision boundary and the closest data point. Since the new added data point is located in the margin the algorithm adjusts its solution by choosing the option with the smallest generalization error. Apparently this solution includes more support vectors (data points within and on the margin boundary).

\subsection{The inverse regularization parameter $C$}

\begin{itemize}

	\item $C$ controls the penalization of data points within the margin. It is scaling data samples so that samples can be misclassified or lie within the margin boundary. This is helpful since not all problems can be clearly separated.

	\item Figure \ref{SVM_Cs} is showing the classification using different values for $C$. The margin boundary gets larger with decreasing values for $C$. As a result the classification is showing more support vectors and also misclassifies the additional data point since these points are less penalized.
	    A very large value for C leads to the strict constraint that the margin borders are denoting the closest data point to the decision boundary. Therefore the margin boundary is very small.

\end{itemize}

\clearpage
\section{Nonlinear (kernel) SVM}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.55\textwidth]{./Figures/2_data.pdf}
\caption{Nonlinear binary classification problem.}
\label{2_data}
\end{figure}

\subsection{Linear kernel}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.65\textwidth]{./Figures/2a_bound_lin}
\caption{Decision boundary obtained by linear kernel.}
\label{2_bound_lin}
\end{figure}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|} \hline
Test score \\ \hline
$81.25\%$\\ \hline
\end{tabular}
\caption{Test score obtained by linear kernel.}
\label{2_score_lin}
\end{table}

\subsection{Polynomial kernel}

\begin{figure}[!ht]
\centering
\begin{subfigure}{0.8\textwidth}
\centering
\includegraphics[width=\textwidth]{./Figures/2b_score_poly}
\caption{Score for varying polynomial degree.}
\label{2_score_poly}
\end{subfigure}

\begin{subfigure}{0.8\textwidth}
\centering
\includegraphics[width=\textwidth]{./Figures/2b_bound_poly}
\caption{Decision boundary for polynomial degree $8$.}
\label{2_bound_poly}
\end{subfigure}
\caption{Results obtained by polynomial kernel.}
\end{figure}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|} \hline
Best degree & Test score \\ \hline
$8$ & $95.75\%$\\ \hline
\end{tabular}
\caption{Highest test score obtained by polynomial kernel.}
\label{2_best_poly}
\end{table}

\subsection{RBF kernel}

\begin{figure}[!ht]
\centering
\begin{subfigure}{0.8\textwidth}
\centering
\includegraphics[width=\textwidth]{./Figures/2c_score_rbf}
\caption{Score for varying $\gamma$.}
\label{2_score_rbf}
\end{subfigure}

\begin{subfigure}{0.8\textwidth}
\centering
\includegraphics[width=\textwidth]{./Figures/2c_bound_rbf}
\caption{Decision boundary for $\gamma=1.85$.}
\label{2_bound_rbf}
\end{subfigure}
\caption{Results obtained by RBF kernel.}
\end{figure}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|} \hline
Best $\gamma$ & Test score \\ \hline
$1.85$ & $94.25\%$\\ \hline
\end{tabular}
\caption{Highest test score obtained by RBF kernel.}
\label{2_best_rbf}
\end{table}

\subsection{Comparison of results obtained by each of the three kernels}

\begin{itemize}
\item While the linear kernel performs expectedly bad on nonlinearly separable data (s. figure \ref{2_bound_lin} and table \ref{2_score_lin}), testing scores obtained by polynomial kernels with degrees $3 \dots 13$ and RBF kernels with $\gamma \geq 1 $ are much higher (s. figures \ref{2_score_poly} and \ref{2_score_rbf}). A polynomial kernel with degree $8$ gives the best results (s. table \ref{2_best_poly}), in figure \ref{2_bound_poly} one can see that the support vectors that are used to produce the decision boundary exclusively lie on the margin boundaries (as opposed to the best RBF result in figure \ref{2_bound_rbf} and table \ref{2_best_rbf}).

\item Although the decision boundary obtained by the linear kernel is the most basic of the three, it takes the most support vectors, which are either misclassified or lie within the margin (s. figure \ref{2_bound_lin}). The polynomial kernel's decision boundary is more complex but at the same time relies on the least support vectors (exclusively on the margin borders, s. figure \ref{2_bound_poly}), because this approach fits the given data best. The most complex decision boundary (with insular patterns, s. figure \ref{2_bound_rbf}) is generated by the RBF kernel with a medium number of support vectors. Rather than only increasing with the complexity of the decision boundary, the number of needed support vectors also decreases the better the kernel fits the data.

\item The RBF kernel generalizes best, as both its training and testing scores continue to rise with increasing $\gamma$, without any overfitting (s. figure \ref{2_score_rbf}). Overfitting occurs when using the polynomial kernel with high degrees ($>>8$, s. figure \ref{2_score_poly})).

\end{itemize}

\clearpage
\section{Multiclass classification}

\subsection{'One vs. Rest' vs. 'One vs. All' algorithms}

SVMs are fundamentally two-class classifier. We can use different approaches to solve problems with $K$ classes. \\
The 'One vs. Rest' algorithm compares the to be determined class (positive) with the rest of the dataset (negative). During the scoring phase the algorithm determines the probability for each class of a sample using the binary classifiers.\\
'One vs. All' creates $K(K-1)/2$ classifiers, which means that we use all possible pairs of classes. Each binary classifiers predicts one class. For classification the class with the highest number of predictions will be chosen. That algorithm requires in comparison to the 'One vs. Rest approach' much more time to get trained and to classify.

\subsection{Classification using linear and RBF}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=.8\textwidth]{./Figures/3a_score.pdf}
	\caption{Scores using linear and RBF kernel on MNIST Dataset}
	\label{multiclass_classification}
\end{figure}

\begin{itemize}

	\item Figure \ref{multiclass_classification} shows that the linear kernel performs very well on MNIST image classification. Using $\gamma = 0,1$ the RBF classification performs slightly better than the linear kernel. For higher $\gamma$ values the classification overfits and the testing scores are quickly descending.
	
	\item The linear kernel performs good, because the sample features which are generated from the MNIST pictures must be very good linear separable in higher dimensions. 

\end{itemize}

\subsection{Confusion matrix}

    \begin{figure}[hbt!]
    \begin{floatrow}
    \ffigbox{%
    \includegraphics[width=.5\textwidth]{./Figures/3b_confusionmatrix.pdf}
    }{%
    \caption{Confusion matrix}%
    \label{confusion_matrix}
	}
	\capbtabbox{%
	\begin{tabular}{|l|l|l|l|l|l|}
	\hline
	  & 1   & 2   & 3   & 4   & 5   \\ \hline
	1 & 147 & 1   & 2   & 0   & 0   \\ \hline
	2 & 3   & 142 & 0   & 4   & 1   \\ \hline
	3 & 4   & 2   & 126 & 0   & 18  \\ \hline
	4 & 3   & 5   & 0   & 142 & 0   \\ \hline
	5 & 2   & 3   & 9   & 0   & 136 \\ \hline
	\end{tabular}
	}{%
	\caption{Confusion matrix}%
	}
	\end{floatrow}
	\end{figure}\\
	
The rows of our confusion matrix correspond to the true classes and the columns to the predicted classes. Class three is the most misclassified class with score of 84\%. 

\newpage

\subsection{Misclassified images}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{./Figures/3b_misclassifieditems.pdf}
	\caption{Misclassified images}
	\label{mnist_misclassified}
\end{figure}

Figure \ref{mnist_misclassified} shows the column of class three. The images are showing the real numbers, which were classified as class three. Apparently the algorithms has problems to distinguish threes and fives. Probably because the lower part of these numbers are very similar. The reason for both falsely determined ones could be that they are untypical ones. The first one is very bold and the other is a bit rotated.

\clearpage
\section{SVM with Gradient Descent}

\subsection{Gradient Descent}

\begin{figure}[!ht]
\parbox[b]{\textwidth}{
\begin{subfigure}{0.6\textwidth}
\includegraphics[width=\textwidth]{./Figures/4_error}
%\caption{Cost over iterations.}
%\label{4_cost}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\begin{tabular}{|c|c|c|} \hline
$\mathbf{w}_{opt}$ & $b_{opt}$ & Final cost\\ \hline
$ \left[\begin{array}{r} -0.2195 \\ 0.2084 \end{array} \right]$ & $0.0025$ & $0.0569$\\ \hline
\end{tabular}
%\caption{Parameters and results.}
%\label{4_param_res}
\end{subfigure}
}
\caption{Results of GD.}
\label{4_GD}
\end{figure}

With the chosen number of iterations $10$ and step size $\eta=0.1$ the GD algorithm approximates the global minimum reasonably well and finds the optimal parameters $\mathbf{w}_{opt}$ and $b_{opt}$ (s. figure \ref{4_GD}).

\subsection{Results of classification}

\begin{figure}[!ht]
\centering
\begin{subfigure}{0.7\textwidth}
\centering
\includegraphics[width=\textwidth]{./Figures/1a_bound}
\caption{Task 1.}
\end{subfigure}
\begin{subfigure}{0.7\textwidth}
\centering
\includegraphics[width=\textwidth]{./Figures/4_bound}
\caption{Task 4.}
\label{4_4}
\end{subfigure}
\caption{Decision functions obtained by SVMs in tasks 1 and 4.}
\label{4_comp}
\end{figure}

Both SVM implementations from task 1 and 4 classify the dataset with an accuracy of $100\%$ (s. figure \ref{4_comp}). The first implementation's decision boundary relies on only two support vectors on the margin. The margin of the SVM implemented in task 4 is larger with samples lying within the margin as well. One reason for this could be the random separation of the data in training and testing sets. In figure \ref{4_4} the closest red point to the decision boundary is part of the testing set and can therefore not be used to generate the decision boundary.

\subsection{Drawbacks of Primal Formulation}

In Dual Formulation the dual coefficients $\alpha_i$ for all samples $i$ are known, i.e. one only needs to sum over the used support vectors to classify new samples. In Primal Formulation one needs to use the whole training set, leading to larger computational cost. Because the $max$-term of the Primal Formulation is non-smooth and therefore not differentiable at $y_i(\mathbf{w}^T\mathbf{x}_i+b) = 1$, subgradients with a conditional function $\mathbb{I}_i$ are needed. Then efficient GD solvers can be used.\\
Additionally - as opposed to Dual Formulation - the Kernel Trick can not be used, making the use of kernels more difficult.
\end{document}}